WEEK 2

Video 1
    * In classification, the dependent variable is discreete
	
Video 2
    * to better determine which k to use in K-NN, divide into folds and apply different k's into each of the folds to then determine which is the best

Video 3
    * How to evaluate the accuracy of classifiers
        * Jaccard index
            * number of correct classifications from the test set divided by number of test samples
        * F1-Score
            * Confusion matrix
            * Precision: (true positive)/(true positive - false positive)
            * Recall: (true positive)/(true positive - false negative)
            * F1-Score: 2*(precision * recall)/(precision + recall)  (best value at 1)
        * Log loss
            * performance of a classifier where the predicted output is a probability value between 0 and 1
            * (y * log(ŷ)) + ((1 - y) * log(1 - ŷ))
            * the classifier with the lower log loss has better accuracy

Video 4
    * How to build a decision tree
        * 1 choose an attribute from the data set
        * calculate the significance of the attribute
        * split the data based on the value of best attribute
        * go to step 1

Video 5
    * in order to calculate the significance of an attribute for building the decision tree
        * choose the attributes with highest purity in the classes divided by its values
        * choose attributes with less entropy in the classes divided by its values
            * if an attribute has two possible values and one of the divisions has the same amount of each class, then the entropy of that division is 1
            * if another division has only values of one class then the entropy is 0

Video 6

Video 7
    
Video 8

Video 9